# An Initial Investigation of ChatGPT Unit Test Generation Capability

Este repositorio contiene los artefactos y cÃ³digo del estudio **"An initial investigation of ChatGPT unit test generation capability"** publicado en SAST'2023.

## ğŸ“‹ Resumen

Este proyecto evalÃºa la capacidad de ChatGPT (GPT-3.5-turbo) para generar pruebas unitarias automÃ¡ticas en Java, comparÃ¡ndolo con herramientas tradicionales de generaciÃ³n de tests como EvoSuite.

## ğŸ¯ Objetivos

1. Analizar el efecto del parÃ¡metro `temperature` en la calidad de los tests generados por ChatGPT
2. Comparar ChatGPT con herramientas tradicionales de generaciÃ³n automÃ¡tica de tests (EvoSuite)
3. Evaluar la efectividad usando mÃ©tricas de cobertura de cÃ³digo y mutation score

## ğŸ“Š MetodologÃ­a

### Conjunto de Datos
- **33 proyectos Java** implementando algoritmos y estructuras de datos clÃ¡sicas
- CategorÃ­as: bÃºsqueda de mÃ¡ximos/mÃ­nimos, ordenaciÃ³n, Fibonacci, listas, pilas, filas, tablas hash, Ã¡rboles binarios, grafos, y algoritmos de casamiento de patrones

### GeneraciÃ³n de Tests
Para cada proyecto:
- **11 configuraciones de temperature** (0.0 a 1.0 con paso de 0.1)
- **3 rÃ©plicas** por configuraciÃ³n
- **Total:** 33 tests por proyecto (11 Ã— 3)
- **Prompt:** "Generate test cases just for the [ClassName] Java class in one java class file with imports using JUnit 4 and Java 8"

### MÃ©tricas Evaluadas
1. **Cobertura de CÃ³digo:** Porcentaje de lÃ­neas ejecutadas por los tests
2. **Mutation Score:** Porcentaje de mutantes detectados (mide efectividad en encontrar bugs)
3. **Tasa de Ã‰xito:** Cantidad de tests que compilan y ejecutan correctamente

## ğŸ“ˆ Resultados Principales

### Efecto de Temperature
| Temperature | Tests VÃ¡lidos | Cobertura | Mutation Score |
|-------------|---------------|-----------|----------------|
| 0.0         | 69            | 46.57%    | 32.25%         |
| 0.5         | 35            | **90.40%**| 67.26%         |
| 0.6         | **52**        | 87.40%    | 68.21%         |
| 0.8         | 45            | 89.78%    | **68.53%**     |

**Hallazgos:**
- Temperature 0.0 genera muchos tests defectuosos (compilaciÃ³n fallida)
- Temperaturas entre 0.5-0.8 producen los mejores resultados
- Temperature 0.5: Mayor cobertura (90.4%)
- Temperature 0.8: Mejor mutation score (68.5%)

### ChatGPT vs EvoSuite
En los 8 proyectos comparados, ChatGPT superÃ³ a EvoSuite en todos los casos:
- **Diferencia promedio:** +5.4 puntos porcentuales en mutation score
- **Rango de ventaja:** +2.6 a +11.6 puntos

### Resultados por Tipo de Algoritmo
- **Algoritmos simples** (Fibonacci, Max/Min): Excelentes (100% mutation score en Fibonacci)
- **Estructuras de datos** (Lista, Pilha): Muy buena cobertura (>97%), mutation score moderado (71-81%)
- **Algoritmos complejos** (CasamentoExato): Alta cobertura (>95%), pero bajo mutation score (28-32%)

## ğŸš€ ImplementaciÃ³n

### Requisitos Previos
```bash
# Requisitos del sistema
- Java 8 o superior
- Maven 3.x
- Python 3.8 o superior
- pip (gestor de paquetes Python)
```

### InstalaciÃ³n

#### 1. Clonar el Repositorio
```bash
git clone <repository-url>
cd chatgpt
```

#### 2. Instalar Dependencias Python
```bash
# Crear entorno virtual (recomendado)
python3 -m venv .venv
source .venv/bin/activate  # En Windows: .venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

#### 3. Verificar InstalaciÃ³n de Java y Maven
```bash
java -version  # Debe ser Java 8+
mvn -version   # Debe ser Maven 3.x
```

### Estructura del Proyecto
```
.
â”œâ”€â”€ projetos/              # 33 proyectos Java evaluados
â”‚   â””â”€â”€ 01Max/
â”‚       â”œâ”€â”€ src/           # CÃ³digo fuente
â”‚       â”‚   â”œâ”€â”€ main/java/ds/      # Clase bajo prueba
â”‚       â”‚   â””â”€â”€ test/java/ds/      # Tests (vacÃ­o inicialmente)
â”‚       â”œâ”€â”€ gpt-tests/     # Tests pre-generados por ChatGPT
â”‚       â”œâ”€â”€ reports/       # Reportes de Pitest (generados al ejecutar)
â”‚       â””â”€â”€ pom.xml        # ConfiguraciÃ³n Maven con Pitest
â”œâ”€â”€ scripts/               # Scripts de generaciÃ³n y evaluaciÃ³n
â”‚   â”œâ”€â”€ gera-chatgpt.py   # Generador de tests con ChatGPT
â”‚   â”œâ”€â”€ reproduce_evaluation.py  # Script de reproducciÃ³n
â”‚   â”œâ”€â”€ reports-chatgpt.py # Extrae mÃ©tricas de reportes HTML
â”‚   â”œâ”€â”€ aggregate_reports.py     # Agrega datos en CSVs
â”‚   â”œâ”€â”€ analyze_metrics_and_anova.py  # AnÃ¡lisis estadÃ­stico
â”‚   â””â”€â”€ files.txt          # Lista de proyectos a evaluar
â”œâ”€â”€ generated_reports/     # Resultados pre-calculados
â”‚   â”œâ”€â”€ all.csv            # Datos brutos
â”‚   â”œâ”€â”€ metrics_by_temperature.csv
â”‚   â””â”€â”€ metrics_by_project_LLM.csv
â”œâ”€â”€ requirements.txt       # Dependencias Python
â””â”€â”€ README.md              # Este archivo
```

### Reproducir la EvaluaciÃ³n (Sin API de OpenAI)

Los tests ya estÃ¡n pre-generados en cada proyecto (`gpt-tests/`). Para ejecutar la evaluaciÃ³n completa:

```bash
cd scripts
python3 reproduce_evaluation.py
```

Este script:
1. Lee la lista de proyectos de `files.txt`
2. Para cada proyecto y cada test (0-33):
   - Copia el test desde `gpt-tests/` a `src/test/java/ds/`
   - Ejecuta `mvn clean install` y Pitest para mutation testing
   - Registra si el test pasa o falla
3. Genera un resumen JSON con estadÃ­sticas

**Nota:** El proceso completo puede tardar **varias horas** (33 proyectos Ã— 34 tests cada uno).

**Tip:** Para probar con un solo proyecto, edita `files.txt` temporalmente.

### Generar Nuevos Tests (Requiere API de OpenAI)

Si deseas generar nuevos tests desde cero:

#### 1. Obtener API Key de OpenAI
RegÃ­strate y obtÃ©n tu API key en https://platform.openai.com/api-keys

#### 2. Configurar la API Key
Edita `scripts/gera-chatgpt.py` y busca la lÃ­nea 23:
```python
"Authorization": "Bearer YOUR_OPENAI_API_KEY_HERE"  # Reemplaza con tu API key
```

Reemplaza `YOUR_OPENAI_API_KEY_HERE` con tu key real.

#### 3. Ejecutar el Generador
```bash
cd scripts
python3 gera-chatgpt.py
```

**Costos:** Ten en cuenta que esto harÃ¡ ~1,000 llamadas a la API de OpenAI (33 proyectos Ã— ~33 tests).

### Analizar Resultados

DespuÃ©s de ejecutar `reproduce_evaluation.py`, puedes analizar los resultados:

```bash
cd scripts

# Generar reportes agregados
python3 reports-chatgpt.py  # Extrae mÃ©tricas de los reportes HTML de Pitest
python3 aggregate_reports.py  # Agrega en CSVs por temperatura y proyecto

# AnÃ¡lisis estadÃ­stico (ANOVA)
python3 analyze_metrics_and_anova.py
```

## ğŸ“ Datos y Resultados

Todos los datos estÃ¡n disponibles en `generated_reports/`:

- **`all.csv`**: Datos completos de 474 ejecuciones exitosas
- **`metrics_by_temperature.csv`**: AgregaciÃ³n por temperatura
- **`metrics_by_project_LLM.csv`**: MÃ©tricas de ChatGPT por proyecto
- **`metrics_by_project.csv`**: ComparaciÃ³n con herramientas tradicionales

## ğŸ” Proyectos Evaluados

| ID | Nombre | Tipo | Complejidad |
|----|--------|------|-------------|
| 01-04 | Max/MaxMin | BÃºsqueda | Baja |
| 05, 09, 17 | Ordenacao | OrdenaciÃ³n | Media |
| 06-07 | Fibonacci | RecursiÃ³n | Baja |
| 11-12 | Lista | Estructura de datos | Media |
| 13-14 | Pilha | Estructura de datos | Media |
| 15-16 | Fila | Estructura de datos | Media |
| 20-23 | Tabela/TabelaHash | Hashing | Alta |
| 21 | ArvoreBinaria | Ãrboles | Alta |
| 24-29 | Grafo | Grafos | Alta |
| 31-32 | Casamento | Pattern matching | Muy Alta |
| 33 | Identifier | ValidaciÃ³n | Media |


**Nota:** Los resultados pueden variar al regenerar tests debido a la naturaleza no determinista de los modelos de lenguaje, incluso con temperature fijo.
